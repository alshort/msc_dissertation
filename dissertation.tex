\documentclass[conference]{IEEEtran}

\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage[nocompress]{cite}
\usepackage{float}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{pdflscape}
\usepackage{pgfgantt}
\usepgflibrary{patterns}
\usepackage{ragged2e}
\usepackage{url}
\usepackage{verbatim}

\newganttchartelement{contingency}{
    contingency/.style={
        shape=rectangle,
        inner sep=0pt,
        draw=black!50,
        very thick,
        pattern=north east lines
    }
}
\lstset{
    basicstyle=\small\ttfamily
}

\begin{document}
\newgeometry{vmargin=2.25cm, hmargin=2.25cm}
\title{GPU-Based Acceleration of Miniapp Performance with CUDA}


\author{\IEEEauthorblockN{Andrew Lamzed-Short}
\IEEEauthorblockA{ID: 1897268}}


\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Mini-applications (``miniapps'') are small-scale, representative versions of large-scale pieces of scientific- or engineering-focused software that seek to model the performance of an algorithm or program without the associated overhead of executing the larger program. This project seeks to examine the effects of altering the code base of one such miniapp called ``SNAP'' from utilising only traditional processing cores to incorporating a mixture of processing cores and graphics devices. Aiming to offload applicable workloads to graphical hardware in order to leverage their increased throughput and capabilities for single instruction, multiple data (SIMD) assignments. The hope is to examine the performance of the resulting program to ascertain whether the modification is beneficial to the runtime of the algorithm, justifying if this change can be reflected in the larger-scale software SNAP represents.

To this end, this dissertation details a brief description to the field of High-Performance Computing (``HPC''), along with an introduction and description of the new field of miniapps. Overall aims of the project are outlined before related work in the field of miniapps, application modelling, and other HPC disciplines and projects are presented. This leads into an investigation into the existing SNAP code base and architecture, a thought out and concise approach and overall design to a solution to this structural challenge, with the process finalised by a documented explanation of the actual implementation. To conclude the report, a testing strategy and results are discussed and analysed, with concluding remarks as to the efficacy and efficiency of the project.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

\subsection{Background}

Modern, frontier-level science calls for large-scale, ambitious projects to answer some of the toughest questions. These projects often involve vast, complex simulations of natural phenomena, from modelling a human brain in one-to-one detail to answer questions about how memory works and how consciousness arises, to modelling the oceans to understand and make predictions about weather and climate change.

One of the predominant questions when designing these simulations is ``what architecture is best to run this program/suite of programs on?''. Different workloads and algorithms are designed for, and benefit from, certain types of computer architecture – some algorithms lend themselves well to being distributed over many cores, whereas others do not. Supercomputers of significant power are leveraged today for the foremost problems of our time: weather simulation and prediction\cite{metoffice}, human brain simulation\cite{humanbrain}, and simulated nuclear weapons testing\cite{nuclear}. The current state-of-the-art supercomputers are published and ranked in a list known as the ``Top500”\cite{top500}, that details their name, location, performance, and power consumption. The most powerful supercomputer to date is``Summit”, housed at Oak Ridge National Laboratory, which can reach an overall performance of 143,500 Tflops/s\footnote{A ``flop” is an abbreviation for 1 floating-point, numerical operation, and a Tflop is a Teraflop, or $10\textsuperscript{12}$ floating point operations.} by utilising 2,397,824 processing cores.

% [Supercomputer Architecture]
In general, supercomputers are comprised of numerous server racks housing many full computer systems, or ``nodes''. Each node may contain several central processing units (CPUs), several graphics cards, many gigabytes of memory, and high-speed networking capabilities allowing every node to be interconnected over high-speed, low latency, high throughput network to allow for communication and cooperation. The topology of the network connecting the computers can vary but two types tend to prevail: computer clusters, and grid computing. Clusters are composed of numerous components that are all connected with a centralised resource management system to act as one individual system, typically on a single site, with multiple clusters connected by a high-speed local area network for low-latency communication. Grid computing utilises clusters that are distributed geographically with the underlying assumption that a user of the system need not worry about where the computing resources they have requested use of are located – this provides reliability, access to and provision of additional resources on demand. The advantage of cluster computing for supercomputing over grid-based computing systems is stability and very low latency between nodes, as there isn’t a need for a high-speed internet connection between sites. An auxiliary benefit to this is that it allows system to be ``air-gapped'' (i.e. completely disconnected) from the outside world for security purposes.

% [OpenMP and MPI – Software and programming paradigms to take advantage of this]
Since the era of Moore’s Law with respect to single-threaded/core workloads is coming to an end\cite{mooreslaw}, modern traditional processors are designed to use multiple cores, with consumer-grade electronics averaging four cores per chip. This is detail can be seen in Figure \ref{fig:cpu_diagram} which details the architecture of a quad-core Intel® Core™ i7 CPU. With the addition of hyper-threading (two threads per physical core), CPUs will have an effective or ``logical” core count of twice the number of physical cores. Programming workloads that take full advantage of this hardware-based parallelism can be challenging to make, and parallelising code over multiple nodes in a supercomputer can be even more so. This is where libraries such as OpenMP\footnote{\url{https://www.openmp.org/}} and MPI\footnote{\url{https://www.mpi-forum.org/}} come in. These are Application Programming Interfaces (APIs) that define how such a complex parallelisation system is to work. Each specification has multiple open-source and proprietary implementations that allow for programmers to convert their code from single-threaded to multi-threaded, multi-process programs. It is these technologies predominantly that a large proportion of HPC applications are built with.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{images/cpu_diagram.png}
\caption{Quad-core Intel Core i7 CPU Architecture Diagram}
\label{fig:cpu_diagram}
\end{figure}

% [Use of GPUs – GPU Architecture]
Graphical Processing Units (GPU) are a newer technology than CPUs. These are dedicated pieces of hardware that serve the purpose of taking instructions from the CPU and performing multiple, hardware-based mathematical operations for translating three-dimensional shapes and coordinates into two-dimensional projections for rendering to a display, alongside other tasks relating to this such as texturing the three-dimensional surfaces and even real-time ray tracing since the Nvidia GTX 20* series. GPUs accomplish some of this by running multiple small programs called ``shaders” to handle colour and lighting. Due to the sheer amount of mathematical calculations that need to be performed to display something on a screen, GPUs have a different architecture to that of a CPU. Modern graphics cards, such as the Nvidia Turing architecture\footnote{\url{https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/}}, pictured in Figure \ref{fig:gpu_diagram}, are composed of multiple stream processors, each divided into hundreds of small cores which perform a single integer or floating-point operation. This stream processing approach allows for vast parallel computation over a large dataset in a paradigm called ``single instruction multiple data” (SIMD).

This parallelism was previously reserved for image and video processing and rendering until a few years ago Nvidia released their CUDA API\cite{cuda_talk}\cite{CUDA} which allows developers to leverage the stream processing nature of the GPU for general-purpose computation. Scientific workloads from biomedical imaging\cite{luebke2008cuda} to deep learning\cite{tang2013deep} are now done on the GPU, and modern supercomputers, such as Summit, are built with large numbers of GPUs to accelerate workloads and perform previously-impossible simulations and workloads.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{images/gpu_diagram.jpg}
\caption{Nvidia Turing GPU Streaming Multiprocessor Architecture Diagram}
\label{fig:gpu_diagram}
\end{figure}

% Miniapps
Mini-applications (``miniapps”) are a new area within the field of High Performance Computing (HPC). These applications are small, self-contained proxies for real applications - typically relating to simulation of physical phenomena - whose job is to quickly and concisely explore a parameter space, leading to focused and interesting performance results to investigate potential scaling and run-time issues or trade-offs\cite{miniapps}. Miniapps capture the behaviour and essence of their parent applications primarily because of two characteristics of distributed applications: the overall performance of an application will mainly be constituted by that of a small subset of the code, and many of the physical models that constitute the rest of the application are mathematically distinct and generally have similar performance characteristics\cite{miniapps}.

\subsection{Objectives}

The SN (Discrete Ordinates) Application Proxy (SNAP) is a miniapp that acts as a proxy for discrete ordinates particle transport. It is modelled on another production simulation program developed by the Los Alamos National Laboratory (``LANL'') called PARTISN, which solves the linear Boltzmann transport equation (TE)\footnote{Boltzmann Equation: \url{https://en.wikipedia.org/wiki/Boltzmann_equation}\raggedright}, simulating neutron criticality and time-independent neutron leakage problems\cite{partisn} in a multi-dimensional phase space. SNAP is a proxy to PARTISN because it provides a concise solution to a discretised, approximated version (though of no real-world relevance) to the same problem PARTISN solves, providing the same data layout, the same number of operations, and loads elements into arrays in approximately the same order.

The SNAP algorithm works by defining the phase space as seven dimensions: three in space (x, y, z), two in angle (octants, angles), one in energy (groups, or energy-based bins of particles), and one of time (time step). SNAP sweeps across the spatial mesh, starting in each of the octants proceeding towards the antipodal octant, performing a time-dependent calculation in each cell using information from the previous time-step and surrounding cells. This ordering forms a wave-front motion that sweeps across the three-dimensional space from corner to corner, with work being divided along each diagonal for parallel execution

With this miniapp in mind, this project defines three key objectives that the solution shall achieve. Taken together, these will provide a holistic overview as to the validity and efficacy of this approach of converting CPU-bound parallelised algorithms to parallelise and accelerate other workloads that utilise GPU also. With the SNAP algorithm and open-source repository (specifically the C-based port of the code) in mind, the three objectives are:

\begin{itemize}

\item To instrument, profile, and analyse the current implementation of the code in order to identify areas of the code in which it would be applicable and beneficial to convert to CUDA-based parallelisation.

\item Using the identified areas found in problem 1, to fork the current C-based port of the SNAP GitHub repository\footnote{\url{https://github.com/lanl/SNAP}} and convert the candidate components and routines from OpenMP to utilise the CUDA libraries instead.

\item Following the integration of CUDA technology into the algorithm, the last aim is to analyse and evaluate the efficiency and efficacy of the new solution in comparison to the previous CPU-based approach. Ideally, a theoretical maximum efficiency of the approach will also be calculated mathematically, and the actual implementation compared against this as another measure of success.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Related Work}

A seminal work in the field of miniapps was written by Heroux et al\cite{miniapps}, defining the paradigm. Their Mantevo miniapp suite has show successful development of miniapps, such as MiniFE for finite element analysis and MiniMD for molecular dynamics simulations, to demonstrate their versatility and applicability. Others have demonstrated such success in other areas, such as Mallinson et al with ``CloverLeaf”\cite{mallinson2013cloverleaf}, and Los Alamos National Lab (\url{https://www.lanl.gov/projects/codesign/proxy-apps/lanl/index.php}). Miniapps have been shown to produce similar performance characteristics to their fully-fledged counterparts\cite{miniapps}, adding to the efficacy of the paradigm.

General-purpose simulations on GPUs have been studied for a long time, with GPUs being a core part of modern computing clusters\cite{debardeleben2013gpu}. Strong-scaling across multiple GPUs\cite{glaser2015strong} is the ideal approach. Consideration is taken also for conversion of existing code bases\cite{zhou2011gpu} and new, bespoke solutions designed with GPU architecture utilisation in mind\cite{glaser2015strong}. Bespoke solutions offer superior code architecture and speed, meaning calculation of theoretical maximum performance increase for a pre-existing code base will have to take this into account.

Writing GPU targeted miniapps in a developing area of work. Baker et al\cite{baker2012high} discuss implementation details of converting the KBA sweep algorithm of the Denovo code system to run on Nvidia Titan GPU. Mallinson et al\cite{mallinson2013cloverleaf} demonstrate too with CloverLeaf the performance advantages GPU-based architecture targeting can have over purely CPU-based versions. It is important to note that these performance increases might not necessarily be completely reflected in SNAP’s algorithm due to other considerations, such as the scaling characteristics of the algorithm\cite{shoukourian2014predicting} and communication technologies as highlighted by Glaser et al\cite{glaser2015strong}.

Performance of miniapps with respect to CPU- and GPU-based parallelisation frameworks have been explored previously and show promising results which add credence to the motivation of this project. Notably Martineau et al\cite{martineau2017productivity} reached the conclusion that compiling miniapps to CUDA resulted in greater efficiencies compared to other targets, though care is needed to consider the implementation (especially with respect to data accesses) to avoid the compiler introducing performance penalties.

Development of the solution must still mimic the behaviour of the original application however, so care must be taken to preserve this. Heroux et al\cite{miniapps} and Messer et al\cite{messer2015developing} outline the fundamental principles that a miniapp must adhere to and the considerations of forming a miniapp from the base application – all of which would help form testing criteria for this project and future projects to help preserve results and intrinsic behaviour.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Project Management}
\label{sec:projectmgmt}

Several factors need to be considered when developing a software project. Management of time constraints is discussed first as producing a novel software project as part of an entire Masters course will run into difficulty. Most of the difficulty will be with conflicting or overlapping deadlines, but also due to the inevitable problems associated with any software development project. Following on from this, it is also key to highlight how the code was managed and versioned so as to preserve the history of changes and the progression of the project, should ideas need to be experimented with or code recovered in the event that anything unforeseen occurs.

\subsection{Time Constraints}

Time constraints are always a pressing and important factor to consider when starting a large project. The external largest constraints imposed on this project would be the time required to complete several overlapping coursework assignments for various MSc modules, in addition to setting aside time to adequately prepare for and take the end of year exams in May and June. To manage these obstacles, the timeline presented in Figure \ref{fig:gantt_chart} was developed early on in the research phase of the project to delineate when and how long certain key stages of development were to last for, with the potential speed-bumps to this project's progression (i.e. MSc examinations) highlighted and accounted for.

\begin{figure*}[ht]
    \centering
    \ganttset{calendar week text=\small{\startday}}
    \begin{ganttchart}[
        y unit chart=0.75cm,
        hgrid,x unit=0.1cm,
        hgrid style/.style={draw=black!5, line width=.75pt},
        milestone/.style={fill=black, draw=white, rounded corners=4pt},
        milestone label font={\it\scriptsize},
        bar label node/.style={text width=2cm,align=right,font=\scriptsize\RaggedLeft,anchor=east},
        group label node/.style={text width=2cm,align=right,font=\bf\scriptsize\RaggedLeft,anchor=east},
        title label anchor/.style={below=-1.6ex},
        time slot format=little-endian]{01-05-2019}{26-09-2019}
    \gantttitlecalendar{month=shortname, week=4} \\
    \ganttgroup{Project Duration}{01-05-2019}{26-09-19}\\
    % Impediments
    \ganttmilestone{Examinations}{13-05-19}
    \ganttmilestone[inline=false]{}{30-05-19}
    \ganttmilestone[inline=false]{}{31-05-19}
    \ganttmilestone[inline=false]{}{01-06-19}
    \ganttmilestone[inline=false]{}{05-06-19}
    \ganttmilestone[inline=false]{}{08-06-19}
    \ganttmilestone[inline=false]{}{05-09-19}\\
    % Investigation
    \ganttgroup{Investigation}{01-05-19}{24-06-19}
    \ganttcontingency[]{}{25-06-19}{02-07-19}\\
    \ganttbar{Preliminary Exploration}{01-05-19}{01-06-19}\\
    \ganttbar{Codebase Examination}{03-05-19}{17-06-19}\\
    \ganttbar{Learning CUDA}{10-06-19}{24-06-19}\\
    % Design
    \ganttgroup{Design}{05-06-19}{31-07-19}
    \ganttcontingency[]{}{01-08-19}{08-08-19}\\
    \ganttbar{Mathematical Model}{05-06-19}{15-07-19} \\
    \ganttbar{Test Harness Concept}{01-07-19}{15-07-19}\\
    \ganttbar{Ideation}{01-07-19}{31-07-19}\\
    % Design and implementation
    \ganttgroup{Implementation}{01-07-19}{18-08-19}
    \ganttcontingency[]{}{19-08-19}{30-08-19}\\
    % Testing
    \ganttgroup{Testing}{15-07-19}{30-08-19}
    \ganttcontingency[]{}{31-08-19}{07-09-19}\\
    \ganttbar{Local Profiling and Benchmarking}{15-07-19}{19-08-19} \\
    \ganttbar{Cluster Profiling and Benchmarking}{29-07-19}{30-08-19} \\
    % Dissertation
    \ganttgroup{Dissertation}{12-08-19}{26-09-19} \\
    \ganttbar{Writing}{12-08-19}{26-09-19} \\
    \ganttbar{Proof Reading and Editing}{5-09-19}{26-09-19}
    \end{ganttchart}
    
    \caption{The Proposed Timeline of this Project}
    \label{fig:gantt_chart}
\end{figure*}

During an early presentation and review of the project, it was made clear that no contingency had been built into the timeline of the project that would alleviate pressure in the event of unfortunate circumstances arriving, such as potentially the unavailability of resources, issues arising during testing, having to adjust the design, or the typical software development issue of features taking longer to develop than initially estimated. Hence, in Figure \ref{fig:gantt_chart}, contingency periods for overall stages of the project are blocked out in the striped areas shown to allow the project to stay on course and on-time.

\subsection{Source Control}

Every major modern software development project uses source control at its core. Source control is key to backing up software in an external repository, maintaining multiple versions of the same code base in a linear or parallelised fashion to be able to restore the code to a former state or maintain a legacy version alongside a more modern version (for instance), and for branching code to allow many developers to work on potentially overlapping areas of the code at once.

To this end, a decision was made to version control the code for this project using \texttt{git}. Out of all of the source control alternatives considered, \texttt{git} has all of the features discussed in addition to being widely supported, an industry standard, and repository hosting services GitHub and GitLab being two of the most popular offerings available. GitHub offers free hosting for public repositories and is already where the original SNAP repository is hosted.

With this in mind, LANL's SNAP repository was forked into a new repository located at \url{https://www.github.com/alshort/snap}. From here it will be cloned onto the local development machine and worked upon in a linear fashion, branching were experimental code is to be created. Tags will be used to denote stable versions when they are created.

In terms of the specific source control methodology used, the project will be managed using an industry standard \texttt{git} branching model called ``git-flow''\cite{gitflow}. The model is illustrated in Figure \ref{fig:gitflow}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth]{images/gitflow.png}
    \caption{Visual map of the git flow methodology}
    \label{fig:gitflow}
\end{figure}

The methodology specifically involves a \texttt{master} branch maintained with only tagged releases, and a \texttt{develop} branch that all of the development is done on. Features and hot-fixes are done on separate branches and merged back into their parent branch, with testing and release-candidate branches spawned off a fixed point on the \texttt{develop} branch. After complete testing and verification that everything works, they will get merged down into the \texttt{master} branch and a new version tagged.

\subsection{Hardware and Software}

Intel's port of SNAP relies on very particular tooling and platform decisions so most of the decisions with regards to testing and development tooling were already determined ahead of time. Furthermore, the decision to utilise CUDA as a platform to accelerate the performance of the miniapp introduced further hardware requirements, necessitating an Nvidia-based GPU with accompanying drivers and development suites. If the modifications to the code base are successful, it is hoped to execute the software under both a local testing environment and a cluster computing environment at the University of Warwick's Department of Computer Science. Details of both of the set-ups follow.

\subsubsection{Local Development Platform}
\label{subsubsec:projmgmt_localtesting}

It is sufficient enough to be able to compile, execute, and test both the stock SNAP application as well as a GPU-accelerated alteration on a typical home desktop computer as of time of writing. Most mid-range desktop computers will have a multi-core Intel CPU as well as a dedicated graphics card. As such, hardware specifications of the home desktop computer used as the local development environment are as follows:

\begin{itemize}
    \item \textbf{Intel® Core™ i7-6700K CPU}: A single quad-core, consumer-grade, overclockable CPU produced by Intel as part of their 6th generation ``Skylake'' line of processors. It has a clock speed of 4GHz (overclockable) and comes as standard with hyper-threading enabled for an (apparent) core count of 8.
    \item \textbf{Nvidia GeForce GTX 1070 graphics card}: Pascal architecture-based GPU that supports the CUDA Compute functionality. In terms of hardware specifications, it has a 1.5GHz base/core clock speed, 8GB of GDDR5 memory, and 15 streaming multi-processors for parallelism.
    \item \textbf{16GB DDR4 RAM}: Fast clock-speed, large storage RAM for storage and retrieval of data.
    \item \textbf{Samsung 860 EVO SSD}: Storage disk for the Ubuntu operating system the project with be developed and tested on. Both the original SNAP porters and cluster environment use a similar distribution of Linux and tools provided with it, so the choice was made mainly for feature-parity and consistency (see below).
\end{itemize}

This set-up allows for the compilation and testing of the current implementation of SNAP as well as a CUDA-accelerated version by both having the correct hardware that the software is and will be built for. Moreover, this set-up is flexible enough in order to test the programs under a range of different inputs and constraints, such as having anywhere from 1 to 8 threads to run the program on. Whilst newer generation Intel® processors would be even more ideal, such as the Intel® Core™ i9-7960X 16-core processor, the current hardware more than suffices enough to yield a satisfactory picture of the performance characteristics of the MPI and OpenMP implementation and limitations for this application.

\subsubsection{Computing Clusters}
\label{subsubsec:clusters}

% Compute nodes: Lenovo NeXtScale nx360 M5 servers with 2 x Intel Xeon E5-2630 v3 2.4 GHz (Haswell) 8-core processors; 16 cores per node; 203 nodes; 3488 cores; 64 GB DDR4 memory per node / 4 GB per core
% GPU nodes: 8 x NVIDIA Tesla K80 GPU cards; 2 GPU cards per node; 4 GPUs per node; 4 nodes; 64 GB DDR4 memory per node
% Phi nodes: 8 x Xeon Phi 7120P; 2 Phi processors per node; 4 nodes; 64 GB DDR4 memory per node
% Fat nodes: 128 x Intel Xeon E7-4809 v3 2.0 GHz Haswell cores; 32 cores per node; 4 nodes; 1 TB DDR3 memory per node; 1 x NVIDIA GRID K2 GPU per node
% Interconnect: QLogic TrueScale InfiniBand
% Storage: 0.5 PB GPFS
% OS: CentOS 6.x
% Workload Manager: Slurm
% Commissioned: Oct 2015
% Expected EOL: Oct 2019
% Intended use: Parallel computing

There are two local high-performance computing clusters available for use at the University of Warwick that are ideal to test the potential multi-GPU capabilities of the end solution. Tinis and Orac are both similar in capabilities but differ in terms of architecture and hardware - Orac being comprised of newer hardware whilst Tinis is expected to reach end-of-life in October 2019\cite{warwickclusters}.

Tinis will be the focus of the cluster-based testing of the GPU-accelerated program because of the more than capable hardware to prove or disprove the effectiveness of the final solution, in addition to increased familiarity with it as a result of the teachings of the CS402 course that is part of the MSc Computer Science program. There are many different types of nodes built into the cluster for handling a wide range of applications: compute nodes, GPU nodes, Phi nodes, and Fat nodes. The Phi and Compute nodes are purely computationally focused so are not appropriate for the needs of this application (in addition to Phi co-processors being deprecated from Intel's MPI library in the 2018 revision\cite{intelmpilib_whatsnew}). GPU nodes will only have the essential hardware in order to operate several graphics devices per node, which again is not appropriate, as this application will need a mixture of CPU and GPU power. Fat nodes have both high-power server processors and Nvidia graphics cards in each node, making them ideal. The specifications for them, as well as the crucial components of the system as a whole, are as follows:

\begin{itemize}
    \item 4 nodes, each composed of:
    \begin{itemize}
        \item 32 x Intel Xeon E7-4809 v3 2.0 GHz Haswell cores
        \item 1TB DDR3 memory
        \item 1 x NVIDIA GRID K2 GPU per node (CUDA capable)
    \end{itemize}
    \item Networking provided by QLogic TrueScale InfiniBand which supports many-gigabit per second transfer speeds between nodes.
    \item CentOS 6.x as the underlying operating system. It's a robust, reliable distribution of Linux that incorporates all of the hardware in the cluster under one management system.
    \item Slurm as the workload manager for submitting the program as a job to any of the available nodes.
\end{itemize}

In total, this provides, in the best-case scenario, a total of 128 traditional processing cores and 4 high-end graphical processing devices (each with two GK104 GPUs\footnote{As per the specification of the device: \url{https://www.nvidia.com/content/grid/pdf/GRID_K2_BD-06580-001_v02.pdf}}). A hybrid program that has multi-CPU and multi-GPU capabilities should be able to leverage a significant portion of the power provided should the threading and data orchestration work.

Having multiple cores for both types of devices will allow for testing over a range of different configurations of traditional-only computation, and a varying amount of hybrid resources to find the optimal amount, as well as any bottlenecks or trends in performance that might appear.

\subsubsection{Software}

Working with an existing and established code base means adhering to existing tool chains where possible in order to correctly compile the software and to have an identical executing environment to run the program in to ensure the program behaves as intended. Thus, most software choices were predetermined for the project. Where nothing was chosen already, the rest of the choices were made due to personal preference, familiarity, or significant advantages were present that made them ideal for their role. All the options are listed below:

\begin{itemize}
    \item \textbf{Ubuntu}: An open-source flavour of Linux favoured by developers for its rich tooling, reliance on shells and terminals and the power they provide, and ability to work at multiple levels (high or low depending upon the circumstances).
    \item \textbf{bash}: A ``shell'' that provides a command-line interface to execute programs and interact with a Linux-based operating system. Useful for simplicity and for automation purposes with shell scripts, calling \texttt{make} files to handle compilation etc.
    \item \textbf{Visual Studio Code}: A cross-platform, automatable text editor with multiple extensions available. Used specifically in this project for its syntax highlighting capabilities for C, C++, Make, Bash, and \LaTeX, as well as executable tasks to automate routines with its built-in terminal, and the latex-workshop extension for dynamically building, parsing, and previewing \LaTeX documents. The tasks developed for this project can be found in the \texttt{.vscode/tasks.json} file located in the source code.
    \item \textbf{mpiicc}: Proprietary compiler for compiling code that uses both the MPI parallelisation specifications and Intel-specific C code.
    \item \textbf{make}: Shell-based program that follows a given set of recipes within a Makefile for compiling multiple dependent parts of source code. Intel provided a makefile to build their SNAP port with their \texttt{mpiicc} compiler. This will further be modified later to incorporate other files and tools.
    \item \textbf{nvcc}: Proprietary compiler produced by Nvidia to compile C++ code that targets the CUDA Compute capabilities of their compatible graphics cards.
    \item \textbf{pdflatex, bibtex}: Back-end programs for Visual Studio Code and latex-workshop to use to built \LaTeX and accompanying bibliography files. Errors and output are parsed and displayed in Visual Studio Code's in-built terminal for an enhanced and compact workflow.
\end{itemize}

Any additional programs used are merely an intrinsic feature of the operating system used. Features like \texttt{ssh} to remote into the Tinis computing cluster, \texttt{git} for fine-tuned version control, and other utilities are found on most development systems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Investigation}
\label{sec:investigation}

All code previously and actively developed by a third-party is legacy code. SNAP, with its various ports, is no different. The original foundation is developed in FORTRAN90 as this language is still used in many areas of HPC. FORTRAN90 has parallel data constructs and parallelism inherently built into the language which does make it suitable for the task. However, in addition to it being a language released in the early 1990s, there are also modern flavours of it, alongside newer languages they may be more up to the task.

In this section, the SNAP code repository is downloaded and examined, alongside notes on the installation, compilation, and execution procedure of running the binary. Profiling steps are taken to understand the program's architecture and its output, and analysis of potential candidate source files for modification are examined.

\subsection{SNAP Repository Cloning and Documentation Review}

Initial plans involved gaining a fundamental understanding of the nature of the algorithms and data structures that the SNAP program utilises in order to effectively emulate and model its larger counterpart program. LANL provide the full FORTRAN90 source code, in addition to several ports into other, different languages, in the GitHub repository located at \url{https://www.github.com/lanl/snap/}. For the purposes of this project, it will be Intel's C-based port that shall be modified and examined due to ease of use for compilation and finding the appropriate tooling (compilers, syntax highlighters etc.) as well as increased familiarity with the language over FORTRAN90.

The entire repository was forked on GitHub for the purposes of modification. It can be found at \url{https://www.github.com/alshort/snap}. All non-C-based ports were removed from the repository in addition to any miscellaneous files and documentation as these were superfluous - the originals could also be referred to at any time. Only the \texttt{qasnap/} and the \texttt{src/} directories of Intel's original code were kept as these would be the only necessary sets of code needed to build, execute, and test SNAP and this project's proposed modifications.

In the main repository, various presentations and documentation are provided that discuss the reasoning behind creating SNAP, background behind PARTISN and the code SNAP was seeking to model, as well as an overview of some of the implementation details of the main constituent of the approach LANL took: the ``sweep'' algorithm. One such piece of documentation describes the wavefront model of the algorithm and how this can be visualised conceptually. A parallel programming schema is detailed that shows how the problem of computing the wavefront is decomposed into many blocks and planes for easier multi-processing.

\subsection{Execution, Profiling, and Investigation of the Code}

Breaking down and assessing the fundamental components and performance of each of the operating parts of a program is key to modifying legacy systems. This process is not without its own challenges.

Often recreating the correct environment for the program to run in is the first major challenge. Many times the documentation for installing will be found lacking with regards to necessary hardware, required installed packages, or simply configuration details such as environmental variables (such as the \texttt{PATH} variable on most operating systems) that are read by the program. It is often said that ``good documentation is good; bad documentation is still fairly good also''. SNAP's is no exception to this as the provided \texttt{README.md} file contains a lot of helpful information with regards to input data, explaining the background of the program, running the program, and sample output. A cursory inspection of Intel's port also shows a lot of in-code comments to explain each stage, also.

Unfortunately, neither document the foundational packages or dependencies required to run the program. Examination of the code and the libraries it uses is required to find them. Intel's MKL library, for example, is one of these large such dependencies that code has. It is a large development toolkit with a long and sometimes involved installation process and particular \texttt{PATH} requirement.

\subsubsection{Compilation}
\label{subsubsec:inv_compilation}

Intel chose to target the C-based port of SNAP towards Intel-based CPUs, hence the decision, as outlined in section \ref{sec:projectmgmt}, to use a machine to locally test the program having an Intel® Core™ i7-6700K CPU. This decision was made due to the original port writers' knowledge of the benefits that the architecture and proprietary improvements could bring to a strictly MPI-based implementation of SNAP. One such proprietary optimisation is the \texttt{-xAVX} compiler flag that was introduced from the ``Sandy Bridge''-line of Intel CPUs onwards that introduced the option to include new instructions (and expanded old instructions) that allowed for operations - specifically fused multiply-accumulate (FMA) operations - for the execution of some SIMD workloads on floating-point data\cite{KanterIntel}. This operation is similar in principle and execution to the SIMD nature of data manipulation on a standard GPU but cannot compare to the speed or scale of that which is obtainable on streaming multi-processors.

One of the main reasons Intel chose C was to utilise their Math Kernel Library (MKL) and Vector Machine Library (VML) of optimised mathematical functions for execution on Intel processors. \texttt{mpiicc} can be compiled with two flags, \texttt{-DUSEMKL} and \texttt{-DUSEVML}, to state whether those libraries are to be used and \texttt{\#ifdef} directives in the code can defines code blocks that utilise the provided functions. \texttt{\#else} directives provide a standard C-based backup implementation of the code if the program is to be targeted to other architectures.

GNU \texttt{make} is used to compile all facets of the program, with the main compiler being \texttt{mpiicc}. Whilst \texttt{gcc} is typically used to compile regular, standard C and C++ code, \texttt{icc} is Intel's C++ compiler that was built in order to specifically optimise code to run on Intel-created architectures. This includes optimisations for how memory is specifically accessed, how thread-based parallelisation is handled with regards to the OpenMP specification, data layout improvements, and support for modern iterations of C++ language specifications\footnote{\url{https://software.intel.com/en-us/c-compilers}}. \texttt{mpiicc} is an HPC-specific version of the \texttt{icc} compiler that utilises message-passing library built for use with the \texttt{icc} program (in particular its C-based capabilities). This library implements the Message Passing Interface (MPI) specification version 3.1\cite{intel-mpi-ref} to allow for bi-directional inter-process communication, regardless of how those processes are mapped to the underlying hardware.

Whilst provided in an operational state, this provision is predicated on the correct resources and dependencies being installed and linked up on the host system. Installation of Intel's MPI libraries is straight-forward but ensuring that they will operate correctly when called can be an issue.

One of the foremost scripts devised when the project started was a bash script that ran several commands in order to register locations and associations of binary files that are a core part of Intel's MPI libraries with the host shell (standard bash in this instance) so that the environment could call upon these when needed by either the compiler or the runtime code. Figure \ref{fig:source_list} shows this script of ``source'' commands. The execution of this script was appended to the environment's \texttt{.bashrc} file in order for it to get executed before each bash shell has finished initialisation, allowing for the references to be available all to time to accelerate testing and development.

\begin{figure}[h]
    \centering
    \begin{lstlisting}[language=bash, breaklines]
#!/bin/sh
. ~/intel/bin/compilervars.sh -arch intel64 -platform linux
. ~/intel/mkl/bin/mklvars.sh intel64
. ~/intel/bin/iccvars.sh -arch intel64 -platform linux
. ~/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh 
    \end{lstlisting}
    \caption{intel.sh}
    \label{fig:source_list}
\end{figure}

There are optional environmental variables that can also be set to exhibit fine-tuned control over the execution profile of the application. Mainly \texttt{OMP\_NUM\_THREADS} to control the maximum number of threads OpenMP can utilise. This value can either default to 1 (most of the time) or the maximum number of threads available to the system.


\subsubsection{Understanding the Input Data}

Provided as part of the original SNAP repository is a folder called \texttt{qasnap}. In this folder resides several input and respective output files for quality assurance and testing. Executing the binary file as above and then comparing it line-by-line to the model output will highlight any discrepancies in the accuracy of the answer - any incorrect modification to the algorithm will flag as an inaccuracy. (The output files also output the timings of the execution but this will be guaranteed to be different on any machine.)

The input data is in a proprietary format in a whitespace-indented file. Due to the fact that the code is a proprietary piece of software developed for a specific task, it is understandable that the documentation is sparse in places, especially with regards to the meaning of these key-value pairs in context.

LANL is in the United States of America, so direct support requests would be costly in terms of time and constrained by time zones, in addition to being potentially low-priority items. Mercifully, within Intel's C-based port contains useful commenting. Hence, alongside analysis and reading of the code, use and meaning of the input variables can be inferred from context for the most part. Figure \ref{table:input-data} shows several of the more essential items. 

\begin{figure}[h]
    \centering
    \begin{tabular}{| p{1.7cm} | p{5.7cm} |}
        \hline
        Variable & Use \\
        \hline
        \texttt{nthreads} & Number of threads per process. \\
        \texttt{npey} & Number of parallel processing inputs in the y direction. \\
        \texttt{npez} & Number of parallel processing inputs in the z direction. \\
        \texttt{ndimen} & Total number of dimensions to the grid. \\
        \texttt{nnested} & Boolean defining whether OpenMP nested threading is active or not.\\
        \texttt{nx} & x dimension of the input grid. \\
        \texttt{ny} & y dimension of the input grid. \\
        \texttt{nz} & z dimension of the input grid. \\
        \texttt{ichunk} & Number of chunk that the i/x direction is divided into. \\
        \texttt{ng} & Number of groups used - introduced by the discretised version of the algorithm. \\
        \hline
    \end{tabular}
    \caption{Important variables defined in the input data}
    \label{table:input-data}
\end{figure}

A mark against the source code is that the variable naming convention is too terse for them to be useful and intuitive. The commenting in every file makes up for some of this but ultimately is not a replacement.

The input data is broadly categorised into three main sections: parallelism controlling variables, dimensionality and initial condition variables, and fine-tuning and control parameters. The first section allows for control of the application to run on different types of hardware. The second section defines size and initial settings but is also flexible in that it can control whether the simulation is performed in two or three dimensions. Whilst this flexibility is great for a general-purpose application, it makes the algorithms and flow more complicated which could make implementation harder. The last section need not be adjusted except in circumstances where the data is the subject being experimented with.

\subsubsection{Execution}

Following successful compilation of the SNAP executable, several executions were performed under various environmental-constraints and input data to ascertain the current limitations and performance of the code base as it stands.

Due to its reliance on MPI for its processing, the compiled program is executed via Intel's \texttt{mpirun} program at the command line with an argument specifying the size of the MPI communication channel (or overall number of parallelised processes), the \texttt{-np} command-line argument, to divide the work into, as opposed to calling the executable by itself. An example execution is as follows:

\begin{lstlisting}[breaklines]
mpirun -np 4 src/snap_mkl_vml --fi qasnap/center_src/in01 --fo test.out
\end{lstlisting}

Figure \ref{fig:snap-successful-run} demonstrates the output of a successful execution of a given input.

\begin{figure}[!h]
    \centering
    \begin{lstlisting}[breaklines]
$ ~/uni/snap/src$ mpirun -np 4 ./snap_cpu --fi ../qasnap/center_src/in01 --fo test.out

*WARNING: PINIT_OMP: NTHREADS>MAX_THREADS; reset to MAX_THREADS
Success! Done in a SNAP!
    \end{lstlisting}
    \caption{A successful execution of the compiled snap\_mkl\_vml binary}
    \label{fig:snap-successful-run}
\end{figure}

It was identified early on into the testing - whilst undocumented, it could be said to be a core feature of the MPI specification itself - that defining an \texttt{-np} value that is not a whole multiple of the product of the defined grid geometry from the input data (\texttt{npey} $ \times $ \texttt{npez}) then the runtime configuration cannot work out how to divide up the workload into whole, viable chunks to send to all processes. Figure \ref{fig:snap-unsuccessful-run} shows the error message provided in this scenario.

\begin{figure}[h]
    \centering
    \begin{lstlisting}[breaklines]
$ mpirun -np 1 ./snap_cpu --fi ../qasnap/center_src/in01 --fo test.out
Abort(202454796) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Cart_create: Invalid argument, error stack:
PMPI_Cart_create(325).....: MPI_Cart_create(comm=0x84000002, ndims=2, dims=0x7ffd07f649b0, periods=0x7ffd07f649b8, reorder=1, comm_cart=0x7ffd07f64bac) failed
MPIR_Cart_create_impl(194): 
MPIR_Cart_create(58)......: Size of the communicator (1) is smaller than the size of the Cartesian topology (4)
    \end{lstlisting}
    \caption{An unsuccessful execution of the snap\_mkl\_vml binary due to imposed multi-processing limitations}
    \label{fig:snap-unsuccessful-run}
\end{figure}

Ascertaining the characteristics, properties, and limitations of an existing code base before modification provides vital insight that can guide the rest of development. Using the limitation with regards to the total of number of processes to run a sample execution on will guide the test plan in particular (Subsection \ref{subsec:testplan}).


\subsection{\texttt{gprof}}

The central issue of profiling parallelised code is that of ascertaining which functions calls are being called and calculating the total amount of time spent there (amongst obtaining other metrics and measurements of interest). In serial code, this task is easy. However, parallel code can be distributed over multiple nodes, processors, and/or threads depending on the resources available, the way the program works, and how the scheduling system decides to allocate work and timeslots. All of this must be orchestrated together, tracked, and aggregated to form one cohesive picture of what the application did on the execution of interest.

\texttt{gprof} is a command-line based profiling tool that displays the call graph profile data of any C, Pascal, or Fortran code it was set to monitor\footnote{On the provision that the code in question was compiled with the \texttt{-pg} parameter beforehand.}. When the compiled binary is run, it will produce a single \texttt{gmon.out} file containing the pertinent profile data. This data is consumed by the main \texttt{gprof} application for analysis. In terms of the output it can produce, \texttt{gprof} is able to relate the \texttt{gmon.out} profile data to the program's symbol table to output a call graph of functions and the functions that call them and how they related to one another. In addition to this, \texttt{gprof} can also output the total number of calls of a function and how long the program spend inside each subroutine.

In order to cater for parallel programs, \texttt{gprof} is able to sum the results of multiple profile data files into one for a general overview. Producing the multiple traces is performed by setting the \texttt{GMON\_OUT\_PREFIX} environment variable to differentiate the traces. Setting this variable to ``\texttt{gmon.out-}'' will cause \texttt{gprof} to append the process's ID to the end of the prefix, producing several files pertaining to the run of the program - one for each process. \texttt{gprof -s} will sum these together and produce a useful, informative result.

Performing this profiling on the standard SNAP application compiled earlier in the investigation yield the table shown in Figure \ref{table:gprof_log}. In the table is a truncated subset of the five most called and time-intensive subroutines. Multiple executions of the program exhibit near identical characteristics in terms of the subroutines reported back by the \texttt{gprof} tool.

\begin{figure}[!h]
    \centering
    \begin{tabular}{ | c | c | c | c | }
\hline
\% Time & Cumulative Secs & Calls & Name \\
\hline
66.67 & 0.02 & 14976 & \texttt{dim3\_sweep} \\
33.33 & 0.02 & 14976 & \texttt{sweep} \\
0.00  & 0.03 & 29952 & \texttt{precv\_d\_3d} \\
0.00  & 0.03 & 29952 & \texttt{psend\_d\_3d} \\
0.00  & 0.03 & 14976 & \texttt{octsweep} \\
\hline
    \end{tabular}
    \caption{Amalgamated \texttt{gprof} log (truncated to top 5 rows)}
    \label{table:gprof_log}
\end{figure}

Two of the functions are associated with intrinsic data transmission across MPI processes. The others are part of the main transport sweep algorithm that is a central tenant of the SNAP program. This was to be expected but is now experimentally confirmed. \texttt{sweep} directly calls \texttt{octsweep} several times in a for loop, using \texttt{precv\_d\_3d} and \texttt{psend\_d\_3d} to synchronise and transfer data across computations (mostly ensuring the boundaries are correct). \texttt{octsweep} in turn directly calls \texttt{dim3\_sweep} after calculation of the location of the chunk to process has been calculated. This process explains the identical number of calls and why the \texttt{precv} and \texttt{psend} functions are called twice as much and the same amount as each other.

Now that a promising candidate for GPU-acceleration has been found in the \texttt{dim3\_sweep} subroutine, a review, analysis, and breakdown of the particular code can be done to identify areas for conversion.


\subsubsection{Code Analysis}
\label{subsec:inv_codeanalysis}

Looking closely at the \texttt{dim3\_sweep.c} code revealed several features about its workings but also highlighted potential trouble areas of development.

Some code in Intel's port was commented out due to compatibility or developmental reasons. Most OpenMP directives were commented out as the original authors could not guarantee they would work on all compilers. Reintroducing these, setting the correct variables, and comparing the time profiles to that of the MPI-only version led to the observation that the hybrid approach was slower than the stock approach on the local development machine. These directives were removed again as a consequence of this.

An abundance of macros introduced by the authors to streamline array and struct data indexing and referencing proved to be a hindrance to the code's readability. Fortunately, in-line macro-replacement is one of the stages of \texttt{gcc}'s pre-processing stage of code compilation. This is achieved with the \texttt{-E} parameter, alongside defining an output file or redirecting the standard output of the shell into a file for examination. The entire pre-processing stage is ran however, meaning the output is some 8000-13000 lines long after all of the library includes. The body of the source code file of interest will always be at the tail end of the file and so simple to extract.

\texttt{dim3\_sweep} is a very interconnected class. It has a relation to most data type definitions which originate and are populated is many disparate places in the code base. Most of the parameters to the method are used routinely in the code, which itself takes the form of a double-nested for loop that performs the diagonal transport sweeps across the data, with many conditionals and smaller, inner for loops controlling variable values.

The shallow nature of the algorithm as it stood didn't immediately lend itself to loop unrolling. A decision was made to attempt to do a partial rewrite of the algorithm in CUDA as the most likely successful integration of CUDA code.

Further up the call stack is \texttt{octsweep} which simply calculates the current \texttt{ichunk} and octant index for the sweep to run on, and \texttt{sweep} one step further up with sequentially calls \texttt{octsweep} for all octants and groups defined at the start. \texttt{sweep} shows promise as a parallelisable higher level to accommodate loop unrolling further down. Finding some way to cascade or overlap the execution of the octants would be beneficial also.

\subsection{Learning CUDA}
\label{subsec:inv_learningcuda}

Being the only way to execute general-purpose computational code on a[n Nvidia] GPU, it is important to comprehensively understand CUDA before delving into any source code modification.

Alongside the content taught in the MSc CS402 High-Performance Computing module, another resource that was used to learn about CUDA was the book ``CUDA by Example'' (ISBN-13: 978-0-13-138768-3) by Jason Sanders and Edward Kandrot\cite{sanders2010cuda}, two senior software engineers working on CUDA teams at Nvidia.

As previously mentioned in Section \ref{sec:introduction}, GPUs are architected in such a way as to maximise the level of parallelism for a given operation over a given set of data at once (a.k.a. SIMD). ``CUDA by Example'' first talks about translating traditional code into kernel functions - small functions that transform some given data only - that operate on the device. Afterwards, concepts and tips are given to help the reader utilise more of the GPU's processing power by introducing the grid-thread-block model, as shown in Figure \ref{fig:gridblockthread}. This abstraction shows how the CUDA runtime divides and distributes work across the inner processors on the graphics device.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth]{images/grid_block_thread.png}
    \caption{Thread block abstraction model used in CUDA programming}
    \label{fig:gridblockthread}
\end{figure}

Each thread and block will have related positional indices that identify it within its parent container. This unique numerical identifier can be decomposed into N-dimensional coordinates (usually two or three dimensional) to perform arithmetic on a specific chunk of the data. Partitioning the data in this way allows for the vast parallelism as the programmer could submit an entire block or a small group of threads to the device for processing - there is a high level of fidelity granted to both data processing and transfer this way.

Before particulars of the CUDA technology are discussed, it's important to note that code that is written for CUDA must be exclusively compiled by the \texttt{nvcc} compiler provided in the CUDA Toolkit. This is the only way that CUDA-C code can get compiled into proper device code. Every other piece of non-CUDA code is ignored by nvcc, instead defaulting to the underlying C++ compiler available on the system - careful consideration of this is needed when attempting to use CUDA in a C-based project. General-purpose CPU code for this project needs to be compiled by the standard C compiler that is a part of the distribution being developed on. This could present issues when trying to use several specialised compilers in tandem to provide a solution that utilises multiple technologies together.

\subsubsection{Parallelisation and Synchronisation}

CUDA at its core is a library for writing very high concurrency code. This immediately brings about problems of how to correct establish and execute the parallel workloads, as well as how to effectively synchronise the tasks to ensure there are no race conditions, overlapping and incorrect memory reads, and correct numerical calculation.

The most powerful concept of CUDA is its use of kernel functions. These are regular subroutines affixed with the \texttt{\_\_global\_\_} keyword. This denotes that they are to be executed on the graphics device. Code decorated with the \texttt{\_\_device\_\_} keyword also only runs on device, and can only be called from \texttt{\_\_global\_\_} and \texttt{\_\_device\_\_} adorned code. This keyword is usually reserved for class-based constructs and supporting device code and algorithms. This kernel function gets executed on each CUDA thread, which each thread being a part of a block which in itself is within a grid (refer to Figure \ref{fig:gridblockthread}). Each grid and block have associated dimension vectors accessed via \texttt{gridDim} and \texttt{blockDim} respectively. Each block and thread have N-dimensional index vectors representing their position in their parent object, accessed with \texttt{blockIdx} and \texttt{threadIdx}.

It's a common pattern to divide the execution of a kernel function $N$ ways for fine-grained control of parallelism. The thread id of the individual executing kernel function in two-dimensional space is defined by the following algorithm:

\begin{lstlisting}[breaklines]
int tid = threadIdx.x + blockIdx.x * blockDim.x; 
\end{lstlisting}

With this calculated, in order to safely perform work, the programmer needs to check that \verb|tid < N|. If so, it's safe to execute the body of the kernel function as it won't cause any out-of-bounds exceptions or similar catastrophic errors.

Often multi-threaded functions will reach a point where all of the threads need to meet at a common instruction to move forwards. This is often the case in map-reduce algorithms where it's important that all mapping functions have finished on every piece of partitioned data before the reduce stage can commence.

For each thread in a block, it is possible to make faster threads hold for slower ones for full synchronisation by inserting the instruction \texttt{\_\_syncthreads()}, an in-built CUDA method, wherever it is possible for every thread to call it. It's vital every thread calls this is or else the program will hang on some threads waiting for others to synchronise when they never will.

\subsubsection{Streams}

Parallelism has a shortfall (or advantage, depending on context) that interleaved, asynchronous operations that execute non-sequentially have no guarantee to execute in any imposed order. Any permutation of all possible interleavings is possible.

Streams are powerful CUDA constructs that allow for managed, controlled parallelism of CUDA functions. Having two streams will allow the same process to occur in down-times between different stages of execution of the same program. For instance, it will be possible for one stream to copy all of its required data into memory buffers and then execute its kernel function. At the same time as the first stream is executing its kernel function, stream 2 can be transferring its data into memory buffers as the data transfer bus is no longer being used by stream 1. This is inherently more useful as previously there would be a bottleneck of memory bandwidth with both memory transfers occurring at the same time.

In the section ``Using Multiple CUDA Streams Effectively'' in ``CUDA by Example''\cite{sanders2010cuda}, the authors recommend, in order to properly schedule work for the GPU in a way that is non-blocking and works according to the flow and inherent premises of the GPU's scheduling algorithm, that like-for-like actions should be done one-after-the-other across all streams instead of by blocks of operations. This leads to a waterfall or cascading effect of operations across all streams. Overall, this method leads to higher performance as the GPU can batch work together and perform several similar operations in parallel.

Concrete understanding and experimentation with this technology could lead to significant, orchestrated performance gains in SNAP also, if utilised correctly. If implemented incorrectly, it could easily bog the execution down. It may be the case that a concept as powerful as streams needs to be deeply integrated into the algorithm's design when the code base is young, or even from its inception, to have maximal effect.


\subsubsection{Memory: Shared, Constant, Texture}

Being largely used for rendering purposes for most of its existence means that the typical graphics card architecture comes with a host of different banks and types of memory and cache that accelerate and speed-up various graphics pipelines and calculations.

Shared memory is one of the most basic forms of memory available in CUDA. Shared memory is named as such as it is a special type of memory that is shared across all threads in a block, but not across blocks. This allows for a form of communication between threads. The memory chips implementing shared memory reside physically on the GPU, so are on-chip rather than off-chip. This proximity to its use allows for conservation of memory bandwidth and low-latency access times.

Shared memory is simply implemented by decorating a variable of choice with the \texttt{\_\_shared\_\_} keyword, as in:

\begin{lstlisting}[breaklines]
__shared__ float cache[N];
\end{lstlisting}

Another performance-improving memory type that could be exploited for this project is \textit{constant memory}. In graphics pipeline, several different processing algorithms, threads, and arithmetic logic units (ALUs) require access to the same scene. Often the frame of the scene is processed and the data and elements within the actual modelled world space can be considered to be unmoving and hence static or constant. To prevent a bottleneck of data, the information required by many different operations can be stored in constant memory on the graphics device. This memory is cached for conservation of memory bandwidth and is treated differently to that of standard memory on the graphics card. Nvidia typically places 64KB of constant memory on graphics card - including the GeForce GTX 1070 card being utilised on the development machine\footnote{Information accurate at of time of writing} - so it is very much similar to the various levels of cache chips found on a traditional processor rather than being akin to RAM. This size of memory will have close proximity and high-bandwidth access to the ALUs on the graphics board. Constant memory is guaranteed by the CUDA runtime to not change for the lifetime of the execution of a kernel function. The one trade-off is this data becomes read-only, so careful management is needs to circumvent this.

Placing data in constant memory is done by prepending the keyword \texttt{\_\_constant\_\_} to the variable in question. Assignment is done separately however. You must \texttt{malloc} or otherwise assign the data using standard C conventions and then call \texttt{cudaMemcpyToSymbol} to copy the data into the \texttt{\_\_constant\_\_} buffer denoted earlier. An invocation would look like this:

\begin{lstlisting}[breaklines]
cudaMemcpyToSymbol(mem, temp_mem, sizeof(Data) * M);
\end{lstlisting}

Where \texttt{mem} is the constant memory, \texttt{temp\_mem} is the standard assigned variable, \texttt{Data} is a data type used for each element, and \texttt{M} is the total size of the collection of \texttt{Data}. It is important to note the restriction that \texttt{sizeof(Data) * M} must be $\leq$ \texttt{64KB}.

Texture memory is a staple memory in graphical processing. It is an on-chip, cached, read-only back of memory that stores texture (bitmap graphics) data used by the texture units on modern graphics devices for sampling and transforming textures for application onto the surfaces of 3D models, as well as for lighting calculations. A particular property that texture memory has that constant memory does not is that of spatial locality of the data. Sampling a part of a texture often requires other threads accessing parts of the neighbouring data for their calculations. Figure \ref{fig:texmem} shows this access pattern.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/TextureMemory.png}
    \caption{Thread-based access of texture memory}
    \label{fig:texmem}
\end{figure}

Naturally, the access procedures and architecture of the memory chips and procedures are designed to use and exploit this. Hence, for two-dimensional data where the value of a cell might rely upon or otherwise update the values of neighbouring cells, utilising this type of memory would gain larger performance increases.

Textures are implemented by the generic \texttt{texture<?>} class, where \texttt{?} is the type of data being stored. Data that requires transfer into the texture, now bound in texture memory, must be allocated on the device, so requires a \texttt{cudaMalloc} call. Transferring data into texture memory from the allocated memory is accomplished with the \texttt{cudaBindTexture} or \texttt{cudaBindTexture2D} (for two-dimensional textures) functions. The only difference between the two is the amount of adjacent neighbours and the texture's layout in memory\cite{wilt2013cuda}.

Textures must be unbound after use with \texttt{cudaUnbindTexture}.

Careful consideration of the use-cases of each type of memory for certain applications, especially ones that could usefully pertain to this project, will be a key consideration during the design stage as to which solutions are feasible and which aren't.


\subsubsection{Multi-GPU Support}

Finally, CUDA has built-in support for the utilisation of multiple graphics devices for a single program. Most high-performance computing clusters will use one or more graphics devices per node in their multi-node clusters, especially for modern applications.

The host operating system will encompass all the collective nodes and resources in them under one umbrella to make the system look like one computer with the aggregated CPUs, GPUs, RAM, and other resources together, all unified. CUDA is able to discern the properties of devices running on the host computer. Calling these functions with multiple graphics devices will return a list of devices and their capabilities, each with their own ID and set of properties. In a homogeneous computing cluster, each node will have the same resources as thus there will be no difference as to what part of the program will be executed on which. However, in a heterogeneous computing environment, it will be advantageous to interrogate each device and allocate appropriate work to appropriate devices. For example, if a workload can only be split up into one large subtask and several supporting ones, a feasible assignment would be to put the fastest or largest device to the task of the large subtask and to distribute the smaller subtasks randomly across the remaining devices. It will all be down to the resources allocated to the submitted job or whatever is available.

Using \texttt{cudaSetDevice(id)} allows for targeting of a thread's kernel function to a particular device. The rest of the function and clean-up procedure remain the same. Knowledge of devices is very proprietary and optimisations may be very specific. A viable design goal worth exploring could be to implement a smart device querying and work scheduling/partitioning system during the initialisation of the application. However, there is no guarantee that this generalised approach would yield significant advantages.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Design}
\label{sec:design}

Principally, the aim of this project is to establish, as a proof-of-concept, whether an existing code base can be meaningfully accelerated using CUDA technologies. A complete rewrite is not the goal as SNAP is a legacy project.

Establishing the efficacy of the proposed solution will be done against a best-case scenario model of parallel execution. Knowing this model is not attainable in practice for this project is acceptable (and not in scope). It will serve as a solid basis for future work should a complete refactor or related work be undertaken.

Following the conceptual, pure model, potential designs are remarked upon with their advantages and disadvantages highlighted, and supporting programs specified afterwards.


\subsection{Mathematical Model}

In order to understand the amount of reasonable work the GPU could handle at once, as well as estimates for the amount of time and data movement needed for the optimal implementation, a mathematical model was developed to help visualise and describe the interactions between adjacent cells in the approximated algorithm that SNAP uses. The following assumptions were made when developing this model:

\begin{enumerate}

\item The algorithm defines the problem space as a three-dimensionally discretised grid of cells, each with their own values and interactions with their neighbouring adjacent cells. This forms the core of the SNAP algorithm (it itself approximating a continuous algorithm).

\item The problem space is cubic with side $ n $ for ease of modelling. A generalisation to a cuboidal space with dimensions $ (x, y, z) $ can be abstracted from the cubic model at a later time. A cubic model is conceptually and mathematically more intuitive to model.

\end{enumerate}

\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{images/Sweep.jpg}
\caption{Slices of a sweep of 3D grid}
\label{fig:3dsweepslice}
\end{figure}

Given these assumptions, we aim to derive the total number of communications between cells in the grid per iteration and how much work this equates to and how much can be parallelised at each stage.

There are $ 8 $ vertices of a cube and thus $ 8 $ octants to perform the sweep over. Each sweep is divided up into diagonal slices, shown in Figure \ref{fig:3dsweepslice}, of which there are $ (x + y + z) - 1 $ of them ($ 3n - 1 $ in this model). A key point to note is the dependency in each sweep of a slice on the slice before as the direction of the sweep is linear and calculations rely upon previous ones\footnote{As described in the documentation of the SNAP algorithm in their GitHub repository - \url{ https://github.com/lanl/SNAP/blob/master/docs/SNAP-overview-presentation-2015.pdf}}. Each combined sweep of all octants, after all results being collated, forms the basis of the next sweep.

The first $ n $ slices of the grid are comprised of the sum of the first $ n $ triangular numbers’ (i.e. $ 1, 3, 6, 10, 15\dots $)\footnote{Sequence A000217 in the ``On-line Encyclopedia of Integer Sequences'' (\url{https://oeis.org/A000217})}, worth of cells, with the final $ n $ slices having the same number also (just in reverse order). $ n - 2 $ slices exist between the 2 triangular-based pyramids formed from the previous step, allowing us to develop a function for the first $ m = n + \frac{n - 2}{2} $ slices, and mirror it to get the number of cells in any slice of the grid.

The number of cells in slice $ i $ of a cube of size $ n $ is thus given by the following formula:

\begin{equation}
cells(i) = \left\{
	\begin{array}{ll}
		tri(i) & \mbox{if } i \le n \\
		tri(i) - 3tri(i - n) & \mbox{if } i \le m \\
		cells(m - ((i - 1)\bmod m)) & \mbox{otherwise}
	\end{array}
\right.
\end{equation}

Where $ tri(i) $ represents the $i$th triangular number, $ 1 \le i \le 3n - 1 $. 

This means poor performance early on due to low numbers of cells per layer and the inter-slice dependency, but will ultimately scale well and to more graphics card given a large grid.

To work out data transfer, we need to figure out the amount of sends and receive actions are performed per slice. We can again work first out up to the first $ m $ slices, and take the mirror image for the rest of the slices, swapping the number of sends per slice for the receives of the mirror slice and vice versa. For the first $ n - 1 $ slices, each cell sends to 3 neighbouring adjacent cells in the next slice. When $ i \ge n $, we need to take into account the ``cut-off'' portions where some cells only send to 2 neighbours, hence:

\begin{equation}
sends(i) = \left\{
	\begin{array}{ll}
		tri(i) * 3 & \mbox{if } i < n \\
		(i\bmod(n - 1)) * 2 + \\
			(cells(i) - i\bmod(n - 1)) * 3 & \mbox{if } i \le m \\
		recvs(2n - i) & \mbox{otherwise}
	\end{array}
\right.
\end{equation}

The total number of receive actions is similar. The first cell has no neighbours to receive from, the second slice has only 1, the rest have three, then after the $n$th slice we need to consider where we only receive from 2 neighbours:

\begin{equation}
recvs(i) = \left\{
	\begin{array}{ll}
		0 & \mbox{if } i = 1 \\
		tri(i) & \mbox{if } i = 2 \\
		tri(i) * 3 & \mbox{if } i \le n \\
		(cells(i) - (2n - i)) * 3 & \mbox{if } i \le m \\
		sends(2n - i) \mbox{otherwise}
	\end{array}
\right.
\end{equation}

Both functions are defined in the domain $ 1 \le i \le 3n - 1 $.

If we let $ msg_s $ be the size of a message to send and $ msg_r $ the size of a message to receive from a neighbour (both in number of bytes), and knowing that the fastest data transfer rate of the PCI Express (PCIe) 2.0 bus connecting the graphics card and CPU together is 500MB/s\cite{PCIe}, we can calculate the time per slice to store data from the GPU as

\begin{equation}
time_s = \frac{sends(i) * msg_s}{5 \times 10^8}
\end{equation}

and to send data to it as

\begin{equation}
time_r = \frac{recvs(i) * msg_r}{5 \times 10^8}
\end{equation}

Aggregating these over all slices, alongside an assumed small, constant kernel function calculation time $ C $ over $ cells(i) $ cells in the slices, yields the best-case calculation time for the current approach. From here, we can interleave all 8 octants for calculation at once (of course managing the number of streaming multi-processors, which, after a point, we'd need to vastly scale hardware or queue work) to improve our throughput. $C$ ultimately depends on the type of graphics card being used.

\subsection{Ideation}
\label{sec:ideation}

Analysis of the code and consideration of the mathematical model behind which the best-case scenario would occur led to a few initial ideas on how the solution would be formed. Several other concepts emerged as development progressed also that are worth noting.


\subsubsection{One-to-one CUDA Conversion}

Foremost was the idea that the original SNAP implementation could be converted and adapted in a straight-forward manner to utilise kernel functions where existing loops and OpenMP directives were used.

Intel integrated a tight and fast parallelisation set-up into their port of C that could span multiple processes and threads without a problem. Distributing the work as they did, deferring segments of the sub-workloads to the GPU seemed like an obvious choice.

This would take place at the lowest levels of the call stack, in the slowest subroutines mostly, to accelerate the performance, as the program can only be as fast as its slowest component.


\subsubsection{Partial Rewrite}

Whilst similar in concept to a one-to-one conversion to CUDA, this approach is slightly different in that any potential candidates for CUDA conversion are extracted and recoded in a new file and their original invocation is replaced by a call to the new code. This system is similar in practice to how a regular programmer would refactor a large legacy code base.

A guiding principle here would be to extract semantically compartmentalised components to break them out for documentation and separation purposes. A visual inspection of the original SNAP code base shows areas of tight coupling and lack of documentation that could build-up to make the project cumbersome to modify or update further into the future.

The aim with this design would be to extract parts of the \texttt{dim3\_sweep} subroutine into kernel functions and device code in a separate \texttt{.cu} file to make it easier to understand and read, and to simplify the CUDA code into small, logical parts. Kernel functions ideally should be small in nature as the whole architecture of streaming multi-processors on graphics cards is to run many small kernel functions in parallel for fast, efficient throughput of data.


\subsubsection{Loop Unrolling}

Supplementary research stemming and overflowing from the research and investigation phases revealed work and advice by Nvidia and works about a technique called ``loop unrolling''. Volkov et al\cite{volkov2011unrolling} showed that flattening out the innermost nested loop structure whilst keeping the outermost one parallelised often has the effect of improving the performance of applications. Stone at al\cite{stone2007accelerating} showed that unrolling a central, deeply nested part of their algorithm led to a speed up of double. This additional performance arises as a result of given each individual thread more work to perform. Ultimately, this has the effect of reducing latency in other areas such as the amount of synchronisation points and data transfer between processes.

The overall architecture of the transport sweep algorithm in SNAP is as follows:

\begin{itemize}
    \item \textit{sweep.c}
    For each octant and group in the ndimen-dimensional grid:
    \begin{itemize}
        \item \textit{octsweep.c}
        Determine which octant and chunk is being calculated
        \begin{itemize}
            \item \textit{dim3\_sweep.c}
            Double-nested for loop calculating the flux
        \end{itemize}
    \end{itemize}
\end{itemize}

Inherently there looks to be an ``unrollable'' structure to this algorithm. This hinges on how amenable \texttt{dim3\_sweep} is to being flattened and decomposed into parts.

A unique issue is presented here that distinguishes this project from problems such as Computational Fluid Dynamics. Instead of each cell in the grid being updated independently at the same time, using information from surrounding cells, the transport sweep algorithm is very dependent on the execution of the previous hyperplane (diagonal slice through the grid, refer back to Figure \ref{fig:3dsweepslice}). This inherently imposes an order to the execution, partially limiting the ability to flatten the code and the parallelisation schema that can be utilised.

\subsubsection{Rewrite}

The only way to guarantee harmony and maximum performance is tight integration of CUDA alongside MPI from the code base's inception. Designing for a particular technology allows for every advantage and facet of the library to be used and paired or timed well with others.

Division of work in the existing SNAP program might change noticeably if the MPI schema were accommodating distribution of work to one or more GPUs per node as opposed to just processors.

This design, whilst comprehensive and conceptually the design with the highest overall obtainable performance increase, was discounted after a small period of consideration due to inherent time constraints.


\subsection{Test Harness}

In order to accelerate the process of testing, analysis, and comparison of data and timings yielded from the runs of the two separate SNAP variants, a decision was made to create a test harness program that will automate the process of running and collecting results.

Automation with regards to testing is key to repeatability, reproducibility, the speed at which results can be obtained, and general accuracy of the results. Hence, for testing of both programs to be effective, the test harness program must meet certain criteria in a holistic process to ensure the project is a success. The overall aims of the program are: to act as a wrapper to run both versions of the program with; to provide high-precision timing of executions; to handle any errors thrown; and to output, compare, and save the results of executions for analysis and reporting.

Firstly, the program should act as a wrapper to the execution of both programs. Manual execution of the programs is slow, can be inaccurate, parameters can be missed accidentally, and executing a sequential string of commands, each varying a little (e.g. only changing the input and output destinations) is tedious. Programmatic control will expedite the testing process and provide extra flexibility should the testing process need to be enhanced or altered in the future.

Further to this is, crucially, accurately timing the execution of both programs. It would be an ineffective solution to the problem if the GPU-accelerated version of SNAP would be routinely slower than the original. Specifically, timing both the entire execution of the program and the internal algorithm/function we're seeking to modify would give an overall impression of the efficacy of the solution. For example, if the algorithm's execution time has decreased but the overall program's execution time has increased due to knock-on factors as a result, then the solution would need further work.

Advanced control could not be achieved if the harness were not able to capture output and errors of the underlying programs that it was calling. It's important to know when errors are found as it might be necessary to halt the problem and report it or continue on regardless depending on what happened - the solution needs to provide this support and subsequently handle it. To this end, obtained the output of both of the programs is a required detail to analyse problems, but most importantly, to compare the final numerical results of the execution on identical inputs to ensure that the results are the same. Inconsistent results mean an improper, incorrect solution. This comparison and analytical process needs to also be mostly automated to save users from wasting time doing it manually and potentially not spotting a discrepancy or difference.

A program that meets these central requirements will act as an effective and useful testing tool for assessing if the end solution meets the objectives of this project.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation}
\label{sec:implementation}

An experimental alteration to an existing code base can often be wrought with challenges. Therefore, a solution to this problem that meets all the objectives outlined earlier must satisfy the following criteria:

\begin{itemize}
    \item \textbf{Accuracy} - Simply put: if the results of the program after the modification do not align with the results of the unmodified program, then the solution is not satisfactory and is thus incorrect. Care and constant ratification must be performed often to ensure this trait is maintained as development occurs.
    \item \textbf{Speed} - A solution that is not better than or comparable to the existing program's performance is worse in practice. In the worst case, the program should be at least as fast.
    \item \textbf{Simplicity} - A solution that is conceptually harder than the existing code is not only not as maintainable but increases the fragility of the algorithm. Code that is improved \textit{and} intuitive can lead to further improvements down the line such as performance gains, reduction of necessary code, or easier parallelisation.
\end{itemize}

Accuracy and speed are easily assessed using simple comparative automations and time-keeping. These were the key motivators for introducing a test harness program into the development and testing cycle (see Subsection \ref{subsec:imp_testharness} for development details). Simplicity is a more subjective metric for evaluating a software-based project. A best-attempt at an objective review of the implementation will be featured in the conclusion (Section \ref{sec:conclusion}). However, personal preference and opinion will always factor into such a review, so the reader can also review the code base provided to make up their own mind.

Development was undertaken iteratively after installation of all of the necessary prerequisites to start programming. The basic cycle mimics aspects of the Agile development cycle\footnote{\url{https://www.agilealliance.org/agile101/}} and other traditional development cycles:

\begin{enumerate}
    \item Partition the current problem into a list of remaining sub-problems.
    \item Write code to solve the current sub-problem.
    \item Test the code.
    \begin{enumerate}
        \item If it works, move forward to step 1.
        \item Else, go back to step 2.
    \end{enumerate}
\end{enumerate}

This project diverges from this slightly in that during the second stage code was written for both the CUDA-based alteration and the test harness. As one was developed, the other was developed and tweaked further. Both were kept in step to ensure accuracy and speed. Implementation details of component of the project follow.


\subsection{Prerequisites}
\label{subsec:imp_prerequisites}

To begin development, in addition to the installation of the necessary Ubuntu packages for C/C++ compilation and python development, it's necessary to install binaries associated with the CUDA development kit and runtime.

A proprietary device driver for Nvidia graphics cards on Ubuntu is required for CUDA code to run on the device. This is provided as part of the ``CUDA Toolkit'' that is freely available to all developers from Nvidia's Developer portal \url{https://developer.nvidia.com/cuda-downloads}. Installation is a matter of following the steps in the installer. It was important to note that the driver provided with the installer is the one guaranteed to work with the CUDA Toolkit, as opposed to the ones provided as part of the operating system vendor's software package ecosystem. Modification of the \texttt{PATH} and \texttt{LD\_LIBRARY\_PATH} to reference the \texttt{/usr/local/cuda-10.1/*}\footnote{The latest CUDA Toolkit version at time of development.} folders sets up the running environment properly and, with careful compilation, now allows for compilation and execution of CUDA code via the \texttt{nvcc} compiler.

The operating graphics driver version provided with the CUDA Toolkit download was version 418.67. It had to be made absolutely imperative that this was the running version of the driver, as on Ubuntu there are multiple different versions one can install. The ones on the development machine were nvidia-driver-418, nvidia-driver-390, and an open-source X.Org X server display driver that the distribution defaults to. Sometimes on boot the system may not be able to load 418 for whatever reason so there are some instances where a reconfiguration under Software \& Updates $\rightarrow$ Additional Drivers and a restart required to get the correct driver running. At this time, it is unknown whether this issue is caused by a bug in the operating system or the underlying hardware. An incorrect running driver will either cause any attempt at running CUDA code to halt because the compiled device code cannot properly access the graphics device (the main job of the driver) or cause aberrant, incorrect results during calculations on the device.

One last prerequisite is to have the Intel MPI libraries installed an operational on the development machine also. This is covered in Subsubsection \ref{subsubsec:inv_compilation}.


\subsection{Test Harness}
\label{subsec:imp_testharness}

Developed in conjunction with the modified SNAP application (Subsection \ref{subsec:imp_mod_snap}), the test harness program is a versatile complementary program that aims to help achieve the objectives of the project by providing the following features:

\begin{itemize}
    \item Acts as a wrapper to both CPU and GPU-augmented versions of the SNAP programs.
    \item Precisely times their execution.
    \item Compares outputs of the programs to gauge consistency.
\end{itemize}

For this supplementary program, Python version 3.7+ will be the main implementation language. The core libraries of Python, in conjunction with its ease of development and portability will allow the program to be developed rapidly and be more intuitive. Version 3.7.3 is used explicitly due to its modernity (it was the latest available distribution at the time of writing) and this script's reliance on the \texttt{time.time\_ns()} function (the use of which is described later).

Three of the main APIs utilised in the program that meet most of the needs of the above requires are the \texttt{os}, \texttt{subprocess}, and \texttt{time} libraries found in Python's standard set of core libraries. These libraries provide tight, low-level, flexible interfaces for working with the host operating system in addition to providing high-precision timing facilities for program comparison and profiling. Further advantages are discussed as the implementation details of each point are outlined in turn.

The primary goal of the test harness program is to act as a wrapper to execute either one of the SNAP programs. This control allows for a fine-level of timing and parameter control that isn't guaranteed with manual execution. Python's \texttt{subprocess} library can interoperate with Ubuntu and execute shell commands specified by strings in the script and have them execute within a subprocess. Not only does this provide exact start and end times of when the underlying subprocess is forked and joined back into the main executing process, but the library provides an extensive API to handle and report errors, output, and stream input for a variety of sources, amongst other features. Specifically, \texttt{subprocess.Popen([\dots])} is utilised as it takes in an array of parameters (the head of which being the program binary name), alongside other parameters to fine-tune inputs, outputs, and environment, and executes it as if it were a shell command. Programmatically inserting input parameters, the path of the SNAP binary - chosen by an argument passed into the \texttt{test.py} script itself - to execute, as well as any additional information and arguments for manipulating the execution or profiling characteristics (see Subsubsection \ref{subsubsec:testapi} for further details). If it is ever the case that written algorithms in the script could be deferred to an existing piece of software or bash script (e.g. \texttt{diff} or \texttt{cmp}) then this could also be used to refactor the program to utilise those instead.

The \texttt{os} library handles difference between operating systems types to make the program as OS-agnostic as possible. Details such as file path separators, reading, and writing are handled through a common API. This allows for easy reading of an input or output file into the program for line-by-line parsing by several other routines and libraries used for the other stages and parts of the script. \texttt{os.remove} is used to delete the extraneous \texttt{gmon.out} files produced by profiling, while \texttt{os.path.join} is used to link scripted parts of paths together to find the output directory and particular files within it for averaging of execution times and comparison of the contents of the files - alongside the \texttt{open(\dots)} function from standard Python.

High-precision timing is provided by the \texttt{time} library: specifically, the \texttt{time.time\_ns()} function. This function was introduced in Python 3.7 when nanoseconds resolution was introduced to the \texttt{time} library\footnote{\url{https://docs.python.org/3.7/whatsnew/3.7.html#pep-564-add-new-time-functions-with-nanosecond-resolution}\RaggedRight}. Precision and accuracy is key when doing time-based profiling specifically, hence the require for the nanosecond time resolution. \texttt{time} previously only had the \texttt{time.time()} function which only returned the number of seconds since a certain date, i.e. Unix epoch time, and this was certainly not appropriate. 

Detailed breakdowns via the use of \texttt{MPI\_Wtime()} to time the start and end times of the \texttt{dim3\_sweep} function and its parent subroutines in the call stack were experimented with. The cost of the intricate implementation of summing all the values from all of the MPI processes outweighed the benefit of the insight it would provide so this was not included in the final build. Use of \texttt{gprof} and other profilers provide similar functionality already.

Output from the file is saved, immediately after execution, in a file with the following naming convention: \texttt{outNN\_cpu\_n4\_1}, where

\begin{itemize}
    \item \textit{NN} is the two-digit number corresponding to the respective input file from the \texttt{qasnap} directory.
    \item \textit{cpu} is interchangeable with ``gpu'' depending upon the particular application that was run.
    \item \textit{n4} denotes the number of MPI processes specified as a parameter.
    \item \textit{1} denotes the id of the run. Subsequent runs of the same configuration will increment this value.
\end{itemize}

This nomenclature uniquely identified the executions and allows for rapid processing during analysis. All of the output files are stored in a separate directory called \texttt{out/} (which is automatically created if it doesn't exist).

Post-processing of the output file generated from the SNAP program is done immediately after execution provided that no error was reported. If the program returned a non-zero exit code, then the program reports this and cleans up immediately afterwards. Post-processing involves stripping the output of the time stamp at which the execution occurred because this is unnecessary information that gets flagged up by comparison programs and algorithms, and splitting of the output file into two halves. The first half contains only the values associated with the numerical calculation and simulation, and the second half, which gets stripped from the original output file and written to a new file of the same name except with the suffix \texttt{.timing}, contains the subroutine and overall times of the execution. This bifurcation was done to simplify comparisons to only the like-for-like data. Numerical values output by the software can instantly be compared (give or take minor affordance in terms of rounding and such) to gauge the accuracy of the solution, whilst the separate timing information can be utilised in a separate program (or manually) to garner an appreciation of the difference in performance between the two programs.

Automatic creation of the output file and its name is a key advantage of the harness program's automations. Using a standardised name and processing procedure allows for substantially easier further processing, validation, and parsing.

% compare
Readily available tools for identifying the similarity between two files is axiomatically needed as manual comparison of the output files the SNAP application gives are detailed. Any inconsistencies or numerical changes might be hard to spot.

A \texttt{compare} command was added to the API of the test harness as a post-execution analysis tool to perform a line-by-line comparison of the two input files specified. The calculation in the final version presents a simplified, line-by-line comparison algorithm that count the amount of lines that differ and offers this as a percentage as its output. 0\% change is identical, any percentage difference under 10\% is near-identical, and any other percentage is different.

If the user specifies the \texttt{-v} option with the \texttt{compare} tool, the application will call the tried-and-tested \texttt{diff} application that is bundled as standard as part of most Linux distributions. This invocation to \texttt{diff} uses the \texttt{-s} and \texttt{-y} parameters to show a side-by-side comparison with the differences between the two highlighted, and gets the program itself to verify whether or not the files are identical as a verification to the test harness's calculation.

% average
Tabulation of timing data for comparison later on during the analysis process needs to take into account variations between runs. The \texttt{average} command was introduced to average the overall time taken per group of related output files (the relation being that the output files all have the same configuration). The average time is simply extracted from the populated \texttt{testing.csv} data file that the \texttt{test} program maintains and appends to after each invocation of the SNAP binary through its run command, and then divided by the number of runs that particular configuration has been executed.

This averaged value is far more representative of the overall time taken for the program with given input and parameters. This value is what will be used in the final testing. If more fine-grained comparison of the timing data is needed, \texttt{test compare} can be invoked on two \texttt{.timing} files to see the differences.


\subsubsection{\texttt{test.py} API}
\label{subsubsec:testapi}

The test harness assumes the form of a program operated solely via the command-line. As such, there is a small set of commands and arguments associated with the various functionalities implemented and described in the design. A reference follows, all of which can be accessed via a terminal by issuing the command \texttt{python test.py -h}.

\begin{itemize}
    \item \textbf{setup}: Establishes the executing environment if not already configured.
    \item \textbf{clean}: Cleans any superfluous or unnecessary files from profiling or previous runs that are otherwise cluttering up the working directory.
    \item \textbf{run}: Executes one of the SNAP binaries in the project using the following arguments:
    \begin{itemize}
        \item \textbf{--cpu}: Makes the program choose the original binary file of the SNAP program. Must define either this flag or \textbf{--gpu}.
        \item \textbf{--gpu}: Makes the program choose the modified, GPU-accelerated binary file of the SNAP program. Must define either this flag or \textbf{--cpu}.
        \item \textbf{-n, --np}: Number of MPI processes to run with.
        \item \textbf{-i, --fi}: Path to the input file to test the program with. 
        \item \textbf{--clean}: Removes profiling data and extraneous information and files from previous runs.
    \end{itemize}
    \item \textbf{average}: Calculates the average time of executions meeting a particular configuration.
    \begin{itemize}
        \item \textbf{-i}: Name (not path) of the input file used that was passed into the SNAP application.
        \item \textbf{-n, --np}: Number of processes the execution ran with
        \item \textbf{--cpu}: Whether the execution was run with the original version of SNAP. Mutually-exclusive option with \textbf{--gpu}.
        \item \textbf{--gpu}: Whether the execution was run with the modified version of SNAP. Mutually-exclusive option with \textbf{--cpu}.
    \end{itemize}
    \item \textbf{compare}: Performs a line-by-line comparison of two input output files from previous runs. Return a percentage difference of the two. Any close to 0\% are reported as ``identical'', less than 10\% ``near-identical'', and anything else is ``different''.
    \begin{itemize}
        \item \textbf{--f1}: Name (not path) of output file to compare.
        \item \textbf{--f2}: Name (not path) of second output file to compare to first.
    \end{itemize}
\end{itemize}

The following global flags are also available for any command:

\begin{itemize}
    \item \textbf{-v, --verbose}: Increase the amount of output of the program for diagnostics.
\end{itemize}


\subsection{Modified SNAP Application}
\label{subsec:imp_mod_snap}

Developing the GPU-accelerations for SNAP first involved understanding how to integrate the CUDA algorithms into the existing MPI-based C implementation.

It was required to get both the \texttt{nvcc} and \texttt{mpiicc} to co-operate with one another, first and foremost.

Secondly, in terms of the process of development, was to attain an understanding of how best to integrate CUDA and MPI code together. An issue presented immediately is that of both the specialised compilers splitting the compilation process into stages. Each compiler will compile separately the specialised parts of the code that the standard C/C++ compiler does not know how to handle, whilst, either at the same time or after the first stage, it will call the standard, underlying compile to compile the rest of the source files. Afterwards, the specialised compile will link both compile non-standard and standard object files together into one executable binary file. Having both compilers do this process means that specialised MPI and CUDA code may not be able to easily co-exist in the same source code files without separation or careful orchestration of the build process.

\subsubsection{\texttt{nvcc} and \texttt{mpiicc}}

As discussed earlier, \texttt{nvcc} and \texttt{mpiicc} are both non-standard code specific compilers that defer to the standard C/C++ compiler once the specialised code they target have been compiled into object code. An issue lies in which compiler either one looks for: \texttt{nvcc} looks for the standard C++ compiler, \texttt{mpiicc} looks for the standard C compiler. This discrepancy caused an issue when trying to link the compiled object files of both compilers together.

During the investigation, the provided Makefile was complete and able to compile the existing project. A few modifications were made to the parameters passed to \texttt{mpiicc}. The full list ended up being \texttt{-O3 -std=c11 -xAVX -qopenmp -parallel -pg -g}. The targeted C version (\texttt{-std}) was updated to c11, \texttt{-pg} was added to allow \texttt{gprof} to profile the code, and \texttt{-g} to allow for increased debugging.

\texttt{-DUSEMKL} and \texttt{-DUSEVML} where removed because they are Intel-proprietary mathematical libraries optimised for use on Intel CPUs. Many of the new processes and algorithms were aiming to be adapted to use on Nvidia GPUs to had to be traded for the standard, non-Intel-optimised code. Any code between \texttt{\#ifdef USEMKL} or \texttt{\#ifdef USEVML} blocks were discarded in \texttt{dim3\_sweep}.

For compilation of the new \texttt{.cu} files, two new variables were added to define the compiler and flags for this file-type:

\begin{lstlisting}[breaklines]
GPUCC=nvcc
GPUFLAGS=-arch=compute_61 -code=sm_61 -m 64 -rdc=true -dc -dlink
\end{lstlisting}

\texttt{nvcc} is the compiler, and the meaning of each of the parameters is:

\begin{itemize}
    \item \textit{-arch=compute\_61} - Specifically targets the GTX 1070 and related device architecture.
    \item \textit{-code=sm\_61} - Defines architecture to optimise and generate device code for, in this case the GTX 1070 and related devices. 
    \item \textit{-m 64} - Targets 64-bit architecture.
    \item \textit{-rdc=true} - Enables generation of relocatable device code.
    \item \textit{-dc} - Compiles each \texttt{.c} and \texttt{.cu} that contains any relocatable device code.
    \item \textit{-dlink} - Links object files with relocatable device code into executable device code.
\end{itemize}

In addition to the following compilation rule that simply takes \texttt{.cu} files as dependencies and compiles them, using the above variables, into \texttt{.o} object files:

\begin{lstlisting}[breaklines]
%.o: %.cu
    $(GPUCC) $(GPUFLAGS) -c $< -o $@
\end{lstlisting}

Separate \texttt{.cu} files are used to separate and easily compile CUDA code from existing MPI code (this is discussed further in the next section).

Linking these files together involves a new compilation rule, shown in Figure \ref{fig:nvccmpiicc}, using \texttt{nvcc} as the linking compiler as it is best suited to integrating the more non-standard CUDA code (MPI code is closer to standard C). It calls \texttt{mpiicc} as the backup compiler, which, in turn, will defer to the standard C compiler on the machine.

\begin{figure}[!h]
    \centering
    \begin{lstlisting}[breaklines]
$(A_TARGET): $(A_OBJS)
    $(GPUCC) -arch=compute_61 -code=sm_61 -m 64 -std=c++11 -ccbin=$(CC) -Xcompiler "-liomp5" $(LDFLAGS) $(LIBS) $^ -o $@
    \end{lstlisting}
    \caption{New make rule for linking MPI and CUDA}
    \label{fig:nvccmpiicc}
\end{figure}

The latest C++ variant is used, alongside targeting of the 64-bit architecture and the GTX 1070-series (and family) graphics device architecture. New here is the inclusion of \texttt{-ccbin} which points to \texttt{mpiicc} as the compiler to refer to. In order to correctly compile with this sub-compiler, the \texttt{-Xcompiler} flag is needed to build with the \texttt{-liomp5} flag to ensure that the OpenMP library is referenced correctly. Fortunately, this is the only thing that's needed to correctly use this. Relevant linker flags for non-standard linking libraries (\texttt{-L/usr/local/cuda/lib64}) as well as CUDA linking variables (\texttt{-lcuda -lcudart}) are used to link all of the object files together into an executable binary. Taken together, this successfully produces a working binary that utilises both CUDA and MPI and OpenMP.

\subsubsection{Externalised CUDA Algorithms}

Mentioned earlier was the fact that CUDA and MPI code need to be compiled in separate files for the compilers to read and process them correctly. Using headers files, it is possible to reference the CUDA function names and call them from within a file that contains standard and/or MPI-specific calls, but the actual implementation needs to be in its on file.

Data type definitions and respective macros were extracted from \texttt{snap.h} into a new header \texttt{snap\_data.h} which was imported in with a conditional statement so that's it's only included once. This allowed it to be referenced separately in a new header file called \texttt{dim3\_sweep\_cuda.h} which defines all of the prototypes for CUDA-specific versions of the existing sweep implementation. 

\texttt{nvcc} utilised the C++ compiler and code natively, \texttt{mpiicc} does not, opting to default to C instead. Whilst similar, the two language are different and compilation of one language with the other's compiler would cause issues. Fixing this just involves, in any \texttt{.cu} file the needs to be compiled and linked, either wrapping or decorating the called code or referenced \texttt{\#include} file with \texttt{extern "C"}. This allows for proper C-like linking style of the compiled object files.


\subsubsection{Introduction of CUDA}

\begin{itemize}
    \item Translated dim3\_sweep to a cuda file
    \item Altered the call in octsweep
    \item Introduced a kernel function
    \item Moved logic into that
    \item cudaMalloc, cudaMemcpy...
\end{itemize}

After much research, tried-and-failed attempts at implementing or thinking of new designs, work started on the partial rewrite design. Beginning implementation of the partial rewrite design involved creating a new blank \texttt{.cu} file that could be compiled with the above rules.

The \texttt{dim3\_sweep} algorithm was transferred in verbatim and renamed \texttt{dim3\_sweep\_cuda} to avoid name clashing. Altering this algorithm is the fundamental step to starting CUDA integration as, if the implementation is to work up to loop unrolling, the first stage is to get the innermost part of the system sorted.

Parts of the algorithm that were decorated with OpenMP directives had these directives moved and their position notes as this would influence the CUDA implementation insofar as which variables to transfer to the kernel function, how a for loop would be divided into N threads instead etc.

The call in \texttt{octsweep} to the previous function was replaced by the new function instead.

Host code cannot immediately call CUDA code as a lot of initialisation is required before a CUDA kernel can be executed. Data must be size allocated and transferred to the graphics device before any work can be done. This process involves introducing the CUDA \texttt{\#include}s and thus compilation issues. It was better to change the new dim3\_sweep\_cuda function into an entry point subroutine that establishes everything and defers to another kernel function to run the algorithm.

Centrally, the idea was to run all of the inner loop iterations simultaneously with synchronisation periods and data transfer to make calculation identical to before. The upper limit of the inner loop was used as the threads per block value in the kernel call:

\begin{lstlisting}[breaklines]
diagonal_loop<<<1, DIAG_1D(d-1).lenc>>>(...);
\end{lstlisting}

Implementation suspended here to start the first round of testing to provide assurances that the solution was on the right course and still producing the same results as before. Unfortunately, deep issues emerged with the implementation. First and foremost was the CUDA parallelisation strategy was wrong and yielded 0 values and single iterations instead of multiple emerged. The complexity and obscurity of the algorithm and port added to the time pressures and ultimately a result-identical implementation couldn't be made in time. This setback is discussed in the reflection section below (Section \ref{sec:reflection}), and the testing plan and profile for this project and future attempts detailed below.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Testing}
\label{sec:testing}

Verification of the modified software will take place in two stages on different sized platforms, as outlined in subsubsections \ref{subsubsec:projmgmt_localtesting} (Local Development Platform) and \ref{subsubsec:clusters} (Computing Clusters) so see if the code scales. The primary objective is to attempt to observe a time-decrease over every problem size in the best case scenario. Other objectives are:

\begin{itemize}
    \item Ensuring the modified solution produces the same results as the original version is all circumstances.
    \item A subsequent secondary performance increase is observed in multi-GPU environments. This is supplementary and not required, as CUDA has particular technologies associated with distributed work across multiple devices.   
\end{itemize}

There are a couple of main variables that would immediately effect the performance of both programs: the number of CPU threads, the amount of MPI processes, and the block and thread configurations of all CUDA-based kernel function calls. Several variations of these will be incorporated into the testing as they will show trends in parallelism that might not otherwise be observed if just a few fixed values are used. For instance, it may be the case that the algorithm doesn't scale well to higher numbers of processes or threads.

\subsection{Test Plan}
\label{subsec:testplan}

To meet these objectives, the following test plan is laid out. Cluster-based testing may be omitted depending upon the results obtained during local testing - it may be the case that there is no apparent performance increase.

\begin{figure}[!h]
    \centering
    \begin{tabular}{|p{0.75cm}|p{0.75cm}|p{0.75cm}|p{0.75cm}|r|r|}
        \hline
        \multicolumn{2}{|l|}{Testing} & \multicolumn{2}{|l|}{Local} &\multicolumn{2}{|l|}{Cluster} \\
        \hline
        Input File & Procs & CPU Time & GPU Time & Compute & Fat \\
        \hline
        in01 & 4 &  &  &  &  \\
        \hline
        in02 & 6 &  &  &  &  \\
        \hline
        in03 & 6 &  &  &  &  \\
        \hline
        in04 & 4 &  &  &  &  \\
        \hline
        in05 & 4 &  &  &  &  \\
        \hline
        in06 & 4 &  &  &  &  \\
        \hline
        in07 & 8 &  &  &  &  \\
        \hline
        in08 & 8 &  &  &  &  \\
        \hline
        in09 & 4 &  &  &  &  \\
        \hline
        in10 & 4 &  &  &  &  \\
        \hline
        in11 & 4 &  &  &  &  \\
        \hline
        in12 & 4 &  &  &  &  \\
        \hline
        in13 & 4 &  &  &  &  \\
        \hline
        in14 & 1 &  &  &  &  \\
        \hline
    \end{tabular}
    \caption{Proposed test plan for the project}
    \label{table:localtestplan}
\end{figure}

The number of processes has to remain fixed as it must always equal \texttt{npey} $\times$ \texttt{npez}. Multiples above or below can confuse the way the communication channels are established and cause initialisation errors. Each of the times will be averaged over at least three runs in order to provide a more accurate picture of the performance for that particular input.

Comparison between each run of the input file will be done to gauge the consistency of the application should an implementation not be suitable or reliable. Comparison between CPU and GPU versions will subsequently gauge the accuracy of the modified version.

Once, and only if, all of the local testing has passed up until this point and there is at least some improvement in the average reported times of the solution, testing can proceed onto the Tinis computing cluster. Cluster-based testing will take place by crafting slurm jobs in such a way that CPU-based executions are run on compute nodes, and GPU-based executions are run on fat nodes. Identical thread and process counts should only highlight, if anything, differences in the processor's performance. This is likely to not be a large contributing factor if there is a difference in time because the two types of nodes have similar specification processors on-board.


\subsection{Local Testing Results}
\label{subsec:testing_local}

Below are the averaged execution times for the original SNAP binary. The times provided are averaged over three runs for fairness and reliability.

\begin{figure}[!h]
    \centering
    \begin{tabular}{|p{0.75cm}|p{0.75cm}|r|}
        \hline
        \multicolumn{2}{|l|}{Testing} & Local \\
        \hline
        Input File & Procs & CPU Time \\
        \hline
        in01 & 4 & 0.765 \\
        \hline
        in02 & 6 & 0.506 \\
        \hline
        in03 & 6 & 0.553 \\
        \hline
        in04 & 4 & 0.776 \\
        \hline
        in05 & 4 & 0.619 \\
        \hline
        in06 & 4 & 0.633 \\
        \hline
        in07 & 8 & 1.025 \\
        \hline
        in08 & 8 & 1.163 \\
        \hline
        in09 & 4 & 6.976 \\
        \hline
        in10 & 4 & 22.290 \\
        \hline
        in11 & 4 & 6.471 \\
        \hline
        in12 & 4 & 20.405 \\
        \hline
        in13 & 4 & 24.268 \\
        \hline
        in14 & 1 & 6.692 \\
        \hline
    \end{tabular}
    \caption{Obtained results from the original SNAP binary}
    \label{table:results}
\end{figure}

The final binary of the GPU-based SNAP program compiles and executes, but does not provide non-zero output and so a comparison is unable to be drawn against the existing version. Compilation and execution with the integrated CUDA functionality shows promise, however. Reasoning and thoughts on the matter are examined and reviewed in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reflection}
\label{sec:reflection}
% Graham Gibbs - Reflective Cycle (1988)

Unfortunately the project didn't turn out to be a success. A lot of knowledge, practical and theoretical, was learned and developed over the course of the time spent on it, however. An opportunity has presented itself at the end of development to gain more of an insight as to what happened and how to best move forward, ensuring a better outcome in the future.

Using a reflective model such as the Gibbs' Reflective Cycle (Figure \ref{fig:gibbscycle}), we may break down and evaluate the issue and then formulate a plan for the next time it has the potential to arise.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.45\textwidth]{images/gibbs-cycle.png}
    \caption{Gibbs' Reflective Cycle}
    \label{fig:gibbscycle}
\end{figure}

All things considered, the major shortfalls of the project were improper analysis of the code base before development, failure to adequately refactor the source code to make variable names more intuitive and the procedures understandable\footnote{It's a common fault of legacy systems that information gets lost and often they're programmed in a way that isn't conducive to future maintenance without a lot of pain.}. This led, in the end, to an unsuccessful modification of the program and no results.

Whilst this is far from ideal, a large amount of knowledge in the areas of programming general-purpose software for use on graphics devices, and high-performance computing in general, has been gained, with the aim to carry this forward into future projects that will utilise these skills.

The field of miniapps is a new one and there will always be the opportunity to revisit this endeavour. A solid testing strategy was in place to confirm and verify the efficacy and efficiency of the solution, and the research and investigatory phase of the project was a success. Cumbersome hurdles such as getting different compilers to interoperate with one another, as well as linking various different object files together into one executable, were difficult, but lessons and techniques have also been gleaned from this experience - ones to take forward and document for others.

Increased tooling and efforts to scrutinise a code base and deployment of established designs will not only help develop a further understanding of the problem for next time, but will pave the way for more in-depth and suitable design choices.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
\label{sec:conclusion}

This project showed the course and cutting-edge nature of miniapps, the latest field in HPC. A good understanding of the problem domain was established and related works in the field comprehensively explored. A proposed GPU-accelerated design for a performance-enhancing modification to SNAP, one such miniapp, was developed given analysis of the code base from time-based profiling.

Many implementation hurdles were overcome, but ultimately the project couldn't be completed. This is not to overshadow the gain in knowledge and understanding that this opportunity brought about, however. Several better choices and steps were reflected upon and a path laid for future endeavours.

Throughout the course of this project, several limitations and improvements have been encountered and noted and are itemised in the following sections.

\subsection{Limitations}
\label{subsec:limitations}

These limitations were either found to be a limitation of the technology, of the existing code, or of the approach taken or the implementation chosen - anything that could be improved if this project were undertaken again.

\begin{itemize}
    \item \textbf{Environment hard to configure}: Installation of Intel's MPI libraries and Nvidia's CUDA Toolkit on a consistent base with the correct drivers and environment variables and link set-up is hard to do correctly the first time. Drivers would occasionally unload, installations would fail half way through, or other myriad hindrances occurring.
    \item \textbf{Very unintuitive coding style}: Intel's port of SNAP heavily utilised short variable names and copious amounts of macros in order to ``simplify'' the process of indexing common array elements or accessing data structures. As a legacy piece of software been experimented on by a third-party, it was very unintuitive to modify, often needing to resort to code analysis tools and tricks to attempt to gain a fraction more understanding of the code.
    \item \textbf{Hybrid approaches like this aren't common}: Guidance on topics such as these are few and far between. There is a scattering of written material, online and offline, that aided with this project. However, especially with regards to linking the various object files together, it was a case of reading the documentation for each compiler and the errors that resulted after each attempt in order to get things to work by hand.

    Debugging and profiling tools and procedures for hybridised projects are difficult too. Individual tools work well, but the combination of them is sometimes found wanting.
\end{itemize}

\subsection{Improvements}

Following on from these limitations, key improvements or remedies for these issues can be found below. Additional improvements are mentioned that would accelerate and improve the process and overall project, also.

\begin{itemize}
    \item \textbf{Texture memory}: One core feature in the CUDA library that wasn't explored due to learning the legacy code base, pursuing alternative designs, and other time constraints, is texture memory and the potential solution and performance benefits it could bring. Texture memory is specialised, cached memory that improves the memory bandwidth of a running parallel application by exploiting the spatial locality of the data in question, as in Figure \label{fig:texmem} shown earlier.        
    
    For restricted transport-sweep problems over two dimensions, texture memory for the implementation of the grid would greatly speed up reads from memory. Each thread can operate a different directional sweep across the same cached texture, synchronise all of the threads, and aggregate the results back before transferring data back to main memory and the CPU. 

    This approach has promise and would be ideal to pursue further for problems like SNAP.

    \item \textbf{Containerisation}: An issue that appeared time and again during the initial investigation stages was the lack of other projects that had tied together MPI and CUDA as this project was seeking to do. Whilst not unusual for new and novel work, the lack of supporting documentation for this use-case proved to be a hindrance to time and slowed parts of the project down. A key piece of future work for this project in the future could be the establishment of a Docker container (a software housing that guarantees consistency of execution no matter what hardware it is run on) to hold binaries and settings for future release and other researchers to use the technologies together, alongside supporting documentation detailing lessons learned during the course of development.

    \item \textbf{Standardised input data format}: Modernising and documenting the input data structure would help to vastly improve the fidelity and flexibility of the program by allowing faster and more intuitive changes to be made without prior knowledge. As it stands, the input format is very proprietary and analysis of the code, particularly \texttt{input.c}, is needed in order to interpret the variables and what they mean in context.
    
    Utilising a modern, global data standard such as JSON or XML over a whitespace dependent ordering would increase the usefulness of input files used for testing. Not only are there high-performance, fast, compact parsing libraries available for these formats but they're also intrinsically easier to document and modify.

    \item \textbf{Rename all variables}: Improving the look and usefulness of variable names, comments, and structure would greatly improve the readability and maintainability of the code.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{ieeetr}
\bibliography{dissertation}

\end{document}